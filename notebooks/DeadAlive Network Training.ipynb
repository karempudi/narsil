{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from narsil.deadalive.datasets import channelStackTrain\n",
    "from narsil.deadalive.modelDev import trainDeadAliveNet\n",
    "from narsil.deadalive.network import CaffeLSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phaseDirectoriesList = ['/home/pk/Documents/trainingData/deadalive/0/','/home/pk/Documents/trainingData/deadalive/1/' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/pk/Documents/trainingData/deadalive/0/',\n",
       " '/home/pk/Documents/trainingData/deadalive/1/']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phaseDirectoriesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = channelStackTrain(phaseDirectoriesList, numUnrolls=2, fileformat='.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024, 40, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['imageSequence'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deadAliveNetBase(nn.Module):\n",
    "\n",
    "    def __init__(self, device, args=None):\n",
    "        super(deadAliveNetBase, self).__init__()\n",
    "        self.device = device\n",
    "        self.args = args\n",
    "        self.learning_rate =  None\n",
    "        self.optimizer = None\n",
    "        self.outputs = None\n",
    "        self.loss_function = nn.BCELoss()\n",
    "\n",
    "    def loss(self, outputs, labels):\n",
    "        return self.loss_function(outputs, labels)\n",
    "\n",
    "    def setup_optimizer(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=0.0005)\n",
    "\n",
    "    def update_learning_rate(self, lr_new):\n",
    "        if self.learning_rate != lr_new:\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr_new\n",
    "            self.learning_rate = lr_new\n",
    "\n",
    "    def step(self, inputs, labels):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.outputs = self(inputs)\n",
    "        loss = self.loss(self.outputs, labels)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.data.cpu().numpy()[0]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = torch.zeros(size=(2, 1, 800, 36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 800, 36])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deadAliveNet(deadAliveNetBase):\n",
    "    \n",
    "    def __init__(self, device, lstm_size=1024, args=None):\n",
    "        super(deadAliveNet, self).__init__(device, args)\n",
    "        \n",
    "        self.device = device\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_state = None\n",
    "        \n",
    "        self.conv = nn.ModuleList([\n",
    "            nn.Conv2d(1, 8, kernel_size=(3, 3), stride=1, padding =1),\n",
    "            nn.Conv2d(8, 16, kernel_size=(3, 3), stride=1, padding=1),\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=1, padding=1),\n",
    "            nn.Conv2d(32, 32, kernel_size=(3, 3), stride=1, padding=1),          \n",
    "        ])\n",
    "        \n",
    "        self.lrn = nn.ModuleList([\n",
    "            nn.LocalResponseNorm(size=4, alpha=0.0001, beta=0.75),\n",
    "            nn.LocalResponseNorm(size=4, alpha=0.0001, beta=0.75)\n",
    "        ])\n",
    "        \n",
    "        self.conv_skip = nn.ModuleList([\n",
    "            nn.Conv2d(8, 2, 1),\n",
    "            nn.Conv2d(16, 4, 1),\n",
    "            nn.Conv2d(32, 8, 1)\n",
    "        ])\n",
    "        \n",
    "        self.prelu_skip = nn.ModuleList([\n",
    "            nn.PReLU(2),\n",
    "            nn.PReLU(4),\n",
    "            nn.PReLU(8)\n",
    "        ])\n",
    "        \n",
    "        self.fc6 = nn.Linear(48800, 1024)\n",
    "        self.lstm1 = CaffeLSTMCell(1024, self.lstm_size)\n",
    "        self.lstm2 = CaffeLSTMCell(1024 + self.lstm_size, self.lstm_size)\n",
    "        \n",
    "        self.fc_out = nn.Linear(self.lstm_size, 6)\n",
    "        \n",
    "    \n",
    "    def forward(self, input, lstm_state=None):\n",
    "        batch_size = input.shape[0]\n",
    "        \n",
    "        \n",
    "        conv1 = self.conv[0](input)\n",
    "        print(f\"Conv1 shape: {conv1.shape}\")\n",
    "        pool1 = F.relu(F.max_pool2d(conv1, (2, 2)))\n",
    "        print(f\"Pool1 shape: {pool1.shape}\")\n",
    "        lrn1 = self.lrn[0](pool1)\n",
    "        print(f\"Lrn1 shape: {lrn1.shape}\")\n",
    "        \n",
    "        # get the pool features into a vector for the final lstm at this spatial \n",
    "        # scale\n",
    "        \n",
    "        conv1_skip = self.prelu_skip[0](self.conv_skip[0](lrn1))\n",
    "        print(f\"Conv1_skip shape: {conv1_skip.shape}\")\n",
    "        # flatten to pool later\n",
    "        conv1_skip_flatten = conv1_skip.view(batch_size, -1)\n",
    "        print(f\"Conv1_skip_flatten shape: {conv1_skip_flatten.shape}\")\n",
    "        \n",
    "        \n",
    "        conv2 = self.conv[1](lrn1)\n",
    "        print(f\"Conv2 shape: {conv2.shape}\")\n",
    "        pool2 = F.relu(F.max_pool2d(conv2, (2, 2)))\n",
    "        print(f\"Pool2 shape: {pool2.shape}\")\n",
    "        lrn2 = self.lrn[1](pool2)\n",
    "        print(f\"Lrn2 shape: {lrn2.shape}\")\n",
    "        \n",
    "        \n",
    "        conv2_skip = self.prelu_skip[1](self.conv_skip[1](lrn2))\n",
    "        print(f\"Conv2_skip shape: {conv2_skip.shape}\")\n",
    "        # flatten to pool later\n",
    "        conv2_skip_flatten = conv2_skip.view(batch_size, -1)\n",
    "        print(f\"Conv2_skip_flatten shape: {conv2_skip_flatten.shape}\")\n",
    "        \n",
    "        conv3 = F.relu(self.conv[2](lrn2))\n",
    "        print(f\"Conv3 shape: {conv3.shape}\")\n",
    "        conv4 = F.relu(self.conv[3](conv3))\n",
    "        print(f\"Conv4 shape: {conv4.shape}\")\n",
    "        \n",
    "        conv4_skip = self.prelu_skip[2](self.conv_skip[2](conv4))\n",
    "        print(f\"Conv4_skip shape: {conv4_skip.shape}\")\n",
    "        conv4_skip_flatten = conv4_skip.view(batch_size, -1)\n",
    "        print(f\"Conv4_skip_flatten shape: {conv4_skip_flatten.shape}\")\n",
    "        \n",
    "        pool4 = F.relu(F.max_pool2d(conv4, (2, 2)))\n",
    "        print(f\"Pool4 shape: {pool4.shape}\")\n",
    "        \n",
    "        pool4_flat = pool4.view(batch_size, -1)\n",
    "        print(f\"Pool4_flat shape: {pool4_flat.shape}\")\n",
    "        \n",
    "        skip_concat = torch.cat([conv1_skip_flatten, conv2_skip_flatten, conv4_skip_flatten, pool4_flat], 1)\n",
    "        print(f\"Skip concat shape: {skip_concat.shape}\")\n",
    "        \n",
    "        fc6 = F.relu(self.fc6(skip_concat))\n",
    "        print(f\"FC6 shape: {fc6.shape}\")\n",
    "        \n",
    "        \n",
    "        if lstm_state is None:\n",
    "            outputs1, state1 = self.lstm1(fc6)\n",
    "            outputs2, state2 = self.lstm2(torch.cat((fc6, outputs1), 1))\n",
    "        else:\n",
    "            outputs1, state1, outputs2, state2 = lstm_state\n",
    "            outputs1, state1 = self.lstm1(fc6, (outputs1, state1))\n",
    "            outputs2, state2 = self.lstm2(torch.cat((fc6, outputs1), 1), (outputs2, state2))\n",
    "\n",
    "        self.lstm_state  = (outputs1, state1, outputs2, state2)\n",
    "        \n",
    "        fc_out = self.fc_out(outputs2)\n",
    "        \n",
    "        print(f\"FC_out shape: {fc_out.shape}\")\n",
    "        return torch.sigmoid(fc_out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = deadAliveNet(device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 shape: torch.Size([2, 8, 800, 36])\n",
      "Pool1 shape: torch.Size([2, 8, 400, 18])\n",
      "Lrn1 shape: torch.Size([2, 8, 400, 18])\n",
      "Conv1_skip shape: torch.Size([2, 2, 400, 18])\n",
      "Conv1_skip_flatten shape: torch.Size([2, 14400])\n",
      "Conv2 shape: torch.Size([2, 16, 400, 18])\n",
      "Pool2 shape: torch.Size([2, 16, 200, 9])\n",
      "Lrn2 shape: torch.Size([2, 16, 200, 9])\n",
      "Conv2_skip shape: torch.Size([2, 4, 200, 9])\n",
      "Conv2_skip_flatten shape: torch.Size([2, 7200])\n",
      "Conv3 shape: torch.Size([2, 32, 200, 9])\n",
      "Conv4 shape: torch.Size([2, 32, 200, 9])\n",
      "Conv4_skip shape: torch.Size([2, 8, 200, 9])\n",
      "Conv4_skip_flatten shape: torch.Size([2, 14400])\n",
      "Pool4 shape: torch.Size([2, 32, 100, 4])\n",
      "Pool4_flat shape: torch.Size([2, 12800])\n",
      "Skip concat shape: torch.Size([2, 48800])\n",
      "FC6 shape: torch.Size([2, 1024])\n",
      "FC_out shape: torch.Size([2, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4985, 0.5016, 0.4990, 0.4997, 0.5022, 0.5068],\n",
       "        [0.4985, 0.5016, 0.4990, 0.4997, 0.5022, 0.5068]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1 shape: torch.Size([2, 8, 800, 36])\n",
      "Pool1 shape: torch.Size([2, 8, 400, 18])\n",
      "Lrn1 shape: torch.Size([2, 8, 400, 18])\n",
      "Conv1_skip shape: torch.Size([2, 2, 400, 18])\n",
      "Conv1_skip_flatten shape: torch.Size([2, 14400])\n",
      "Conv2 shape: torch.Size([2, 16, 400, 18])\n",
      "Pool2 shape: torch.Size([2, 16, 200, 9])\n",
      "Lrn2 shape: torch.Size([2, 16, 200, 9])\n",
      "Conv2_skip shape: torch.Size([2, 4, 200, 9])\n",
      "Conv2_skip_flatten shape: torch.Size([2, 7200])\n",
      "Conv3 shape: torch.Size([2, 32, 200, 9])\n",
      "Conv4 shape: torch.Size([2, 32, 200, 9])\n",
      "Conv4_skip shape: torch.Size([2, 8, 200, 9])\n",
      "Conv4_skip_flatten shape: torch.Size([2, 14400])\n",
      "Pool4 shape: torch.Size([2, 32, 100, 4])\n",
      "Pool4_flat shape: torch.Size([2, 12800])\n",
      "Skip concat shape: torch.Size([2, 48800])\n",
      "FC6 shape: torch.Size([2, 1024])\n",
      "FC_out shape: torch.Size([2, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "deadAliveNet                             --                        --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-1                       [2, 8, 800, 36]           80\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─LocalResponseNorm: 2-2            [2, 8, 400, 18]           --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─Conv2d: 2-3                       [2, 2, 400, 18]           18\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─PReLU: 2-4                        [2, 2, 400, 18]           2\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-5                       [2, 16, 400, 18]          1,168\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─LocalResponseNorm: 2-6            [2, 16, 200, 9]           --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─Conv2d: 2-7                       [2, 4, 200, 9]            68\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─PReLU: 2-8                        [2, 4, 200, 9]            4\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─Conv2d: 2-9                       [2, 32, 200, 9]           4,640\n",
       "│    └─Conv2d: 2-10                      [2, 32, 200, 9]           9,248\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─Conv2d: 2-11                      [2, 8, 200, 9]            264\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─PReLU: 2-12                       [2, 8, 200, 9]            8\n",
       "├─Linear: 1-5                            [2, 1024]                 49,972,224\n",
       "├─CaffeLSTMCell: 1-6                     [2, 1024]                 --\n",
       "│    └─Linear: 2-13                      [2, 1024]                 2,098,176\n",
       "│    └─Linear: 2-14                      [2, 1024]                 3,146,752\n",
       "│    └─Linear: 2-15                      [2, 1024]                 3,146,752\n",
       "│    └─Linear: 2-16                      [2, 1024]                 3,146,752\n",
       "├─CaffeLSTMCell: 1-7                     [2, 1024]                 --\n",
       "│    └─Linear: 2-17                      [2, 1024]                 3,146,752\n",
       "│    └─Linear: 2-18                      [2, 1024]                 4,195,328\n",
       "│    └─Linear: 2-19                      [2, 1024]                 4,195,328\n",
       "│    └─Linear: 2-20                      [2, 1024]                 4,195,328\n",
       "├─Linear: 1-8                            [2, 6]                    6,150\n",
       "==========================================================================================\n",
       "Total params: 77,265,042\n",
       "Trainable params: 77,265,042\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 227.38\n",
       "==========================================================================================\n",
       "Input size (MB): 0.23\n",
       "Forward/backward pass size (MB): 8.67\n",
       "Params size (MB): 309.06\n",
       "Estimated Total Size (MB): 317.96\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net, input_size=(2, 1, 800, 36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14400"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400 *36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting dead-alive probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xdata, ydata = [], []\n",
    "ln, = plt.plot([], [], 'ro')\n",
    "\n",
    "def init():\n",
    "    ax.set_xlim(0, 2*np.pi)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    return ln,\n",
    "\n",
    "def update(frame):\n",
    "    xdata.append(frame)\n",
    "    ydata.append(np.sin(frame))\n",
    "    ln.set_data(xdata, ydata)\n",
    "    return ln,\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.linspace(0, 2*np.pi, 128),\n",
    "                    init_func=init, blit=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParameters = {\n",
    "    'device': \"cuda:1\"\n",
    "}\n",
    "optimizationParameters = {\n",
    "    'learning_rate': 0.5e-5,\n",
    "    'nEpochs': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = trainDeadAliveNet(phaseDirectoriesList, modelParameters, optimizationParameters, fileformat='.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- started\n",
      "Epoch average loss: 0.15325021594762803\n",
      "Epoch 1 -- started\n",
      "Epoch average loss: 0.1096721351146698\n",
      "Epoch 2 -- started\n",
      "Epoch average loss: 0.11148410886526108\n",
      "Epoch 3 -- started\n",
      "Epoch average loss: 0.10040972828865051\n",
      "Epoch 4 -- started\n",
      "Epoch average loss: 0.09695472568273544\n",
      "Epoch 5 -- started\n",
      "Epoch average loss: 0.09909523427486419\n",
      "Epoch 6 -- started\n",
      "Epoch average loss: 0.09453219920396805\n",
      "Epoch 7 -- started\n",
      "Epoch average loss: 0.09195844233036041\n",
      "Epoch 8 -- started\n",
      "Epoch average loss: 0.09463360160589218\n",
      "Epoch 9 -- started\n",
      "Epoch average loss: 0.09367366284132003\n",
      "Epoch 10 -- started\n",
      "Epoch average loss: 0.0925093412399292\n",
      "Epoch 11 -- started\n",
      "Epoch average loss: 0.09434753209352494\n",
      "Epoch 12 -- started\n",
      "Epoch average loss: 0.09306800067424774\n",
      "Epoch 13 -- started\n",
      "Epoch average loss: 0.09562535881996155\n",
      "Epoch 14 -- started\n",
      "Epoch average loss: 0.10037241280078887\n",
      "Epoch 15 -- started\n",
      "Epoch average loss: 0.10584187507629395\n",
      "Epoch 16 -- started\n",
      "Epoch average loss: 0.09625648856163024\n",
      "Epoch 17 -- started\n",
      "Epoch average loss: 0.09045737981796265\n",
      "Epoch 18 -- started\n",
      "Epoch average loss: 0.09388227015733719\n",
      "Epoch 19 -- started\n",
      "Epoch average loss: 0.0924113392829895\n",
      "Epoch 20 -- started\n",
      "Epoch average loss: 0.08980592638254166\n",
      "Epoch 21 -- started\n",
      "Epoch average loss: 0.07495343238115311\n",
      "Epoch 22 -- started\n",
      "Epoch average loss: 0.07853820994496345\n",
      "Epoch 23 -- started\n",
      "Epoch average loss: 0.07135899588465691\n",
      "Epoch 24 -- started\n",
      "Epoch average loss: 0.07198193222284317\n",
      "Epoch 25 -- started\n",
      "Epoch average loss: 0.07453950345516205\n",
      "Epoch 26 -- started\n",
      "Epoch average loss: 0.07154019996523857\n",
      "Epoch 27 -- started\n",
      "Epoch average loss: 0.06796311512589455\n",
      "Epoch 28 -- started\n",
      "Epoch average loss: 0.0672855667769909\n",
      "Epoch 29 -- started\n",
      "Epoch average loss: 0.06941899061203002\n",
      "Epoch 30 -- started\n",
      "Epoch average loss: 0.0730777770280838\n",
      "Epoch 31 -- started\n",
      "Epoch average loss: 0.0685605488717556\n",
      "Epoch 32 -- started\n",
      "Epoch average loss: 0.07038231939077377\n",
      "Epoch 33 -- started\n",
      "Epoch average loss: 0.06579076275229453\n",
      "Epoch 34 -- started\n",
      "Epoch average loss: 0.06625591367483138\n",
      "Epoch 35 -- started\n",
      "Epoch average loss: 0.07207902893424034\n",
      "Epoch 36 -- started\n",
      "Epoch average loss: 0.07038253173232079\n",
      "Epoch 37 -- started\n",
      "Epoch average loss: 0.07259512394666671\n",
      "Epoch 38 -- started\n",
      "Epoch average loss: 0.06856067478656769\n",
      "Epoch 39 -- started\n",
      "Epoch average loss: 0.07095287069678306\n",
      "Epoch 40 -- started\n",
      "Epoch average loss: 0.07005911692976952\n",
      "Epoch 41 -- started\n",
      "Epoch average loss: 0.06817694902420043\n",
      "Epoch 42 -- started\n",
      "Epoch average loss: 0.07978316098451614\n",
      "Epoch 43 -- started\n",
      "Epoch average loss: 0.12478392720222473\n",
      "Epoch 44 -- started\n",
      "Epoch average loss: 0.0776776947081089\n",
      "Epoch 45 -- started\n",
      "Epoch average loss: 0.0822545662522316\n",
      "Epoch 46 -- started\n",
      "Epoch average loss: 0.13623299300670624\n",
      "Epoch 47 -- started\n",
      "Epoch average loss: 0.09294828921556472\n",
      "Epoch 48 -- started\n",
      "Epoch average loss: 0.0857807606458664\n",
      "Epoch 49 -- started\n",
      "Epoch average loss: 0.06822066679596901\n",
      "Epoch 50 -- started\n",
      "Epoch average loss: 0.06776070594787598\n",
      "Epoch 51 -- started\n",
      "Epoch average loss: 0.06232103183865547\n",
      "Epoch 52 -- started\n",
      "Epoch average loss: 0.06544016450643539\n",
      "Epoch 53 -- started\n",
      "Epoch average loss: 0.0692647747695446\n",
      "Epoch 54 -- started\n",
      "Epoch average loss: 0.060074751079082486\n",
      "Epoch 55 -- started\n",
      "Epoch average loss: 0.057463467866182324\n",
      "Epoch 56 -- started\n",
      "Epoch average loss: 0.060164802521467206\n",
      "Epoch 57 -- started\n",
      "Epoch average loss: 0.05818078741431236\n",
      "Epoch 58 -- started\n",
      "Epoch average loss: 0.05639857575297356\n",
      "Epoch 59 -- started\n",
      "Epoch average loss: 0.060433230549097064\n",
      "Epoch 60 -- started\n",
      "Epoch average loss: 0.058781518042087554\n",
      "Epoch 61 -- started\n",
      "Epoch average loss: 0.059362989850342274\n",
      "Epoch 62 -- started\n",
      "Epoch average loss: 0.056803700514137745\n",
      "Epoch 63 -- started\n",
      "Epoch average loss: 0.056151109747588634\n",
      "Epoch 64 -- started\n",
      "Epoch average loss: 0.05431901849806309\n",
      "Epoch 65 -- started\n",
      "Epoch average loss: 0.053973883390426636\n",
      "Epoch 66 -- started\n",
      "Epoch average loss: 0.05427830293774605\n",
      "Epoch 67 -- started\n",
      "Epoch average loss: 0.05385016184300184\n",
      "Epoch 68 -- started\n",
      "Epoch average loss: 0.05086448322981596\n",
      "Epoch 69 -- started\n",
      "Epoch average loss: 0.051016414538025856\n",
      "Epoch 70 -- started\n",
      "Epoch average loss: 0.05094281118363142\n",
      "Epoch 71 -- started\n",
      "Epoch average loss: 0.05131437350064516\n",
      "Epoch 72 -- started\n",
      "Epoch average loss: 0.053243436850607395\n",
      "Epoch 73 -- started\n",
      "Epoch average loss: 0.05455936957150698\n",
      "Epoch 74 -- started\n",
      "Epoch average loss: 0.05244505126029253\n",
      "Epoch 75 -- started\n",
      "Epoch average loss: 0.05248296167701483\n",
      "Epoch 76 -- started\n",
      "Epoch average loss: 0.050335923209786415\n",
      "Epoch 77 -- started\n",
      "Epoch average loss: 0.0494014210999012\n",
      "Epoch 78 -- started\n",
      "Epoch average loss: 0.04935617558658123\n",
      "Epoch 79 -- started\n",
      "Epoch average loss: 0.049956681206822395\n",
      "Epoch 80 -- started\n",
      "Epoch average loss: 0.05615737661719322\n",
      "Epoch 81 -- started\n",
      "Epoch average loss: 0.048863623291254044\n",
      "Epoch 82 -- started\n",
      "Epoch average loss: 0.046871853061020374\n",
      "Epoch 83 -- started\n",
      "Epoch average loss: 0.046092135831713676\n",
      "Epoch 84 -- started\n",
      "Epoch average loss: 0.04515619669109583\n",
      "Epoch 85 -- started\n",
      "Epoch average loss: 0.045786045491695404\n",
      "Epoch 86 -- started\n",
      "Epoch average loss: 0.046304138377308846\n",
      "Epoch 87 -- started\n",
      "Epoch average loss: 0.04588518105447292\n",
      "Epoch 88 -- started\n",
      "Epoch average loss: 0.04530139360576868\n",
      "Epoch 89 -- started\n",
      "Epoch average loss: 0.046418385580182076\n",
      "Epoch 90 -- started\n",
      "Epoch average loss: 0.04813651833683252\n",
      "Epoch 91 -- started\n",
      "Epoch average loss: 0.04470819979906082\n",
      "Epoch 92 -- started\n",
      "Epoch average loss: 0.04762738570570946\n",
      "Epoch 93 -- started\n",
      "Epoch average loss: 0.050463263876736164\n",
      "Epoch 94 -- started\n",
      "Epoch average loss: 0.05380856338888407\n",
      "Epoch 95 -- started\n",
      "Epoch average loss: 0.05073457397520542\n",
      "Epoch 96 -- started\n",
      "Epoch average loss: 0.04986877180635929\n",
      "Epoch 97 -- started\n",
      "Epoch average loss: 0.04641815461218357\n",
      "Epoch 98 -- started\n",
      "Epoch average loss: 0.04287864547222853\n",
      "Epoch 99 -- started\n",
      "Epoch average loss: 0.04437540378421545\n"
     ]
    }
   ],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
