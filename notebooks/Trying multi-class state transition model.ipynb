{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd8bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0603918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.widgets import Slider, Button, CheckButtons, RadioButtons\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from skimage import io\n",
    "import pickle\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6411b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_to_classes = {\n",
    "    'all_growing': 0,\n",
    "    'partly_growing': 1,\n",
    "    'stopped_growing': 2,\n",
    "    'stopped_fading': 3,\n",
    "    'stopped_vanishing': 4,\n",
    "    'channel_empty': 5\n",
    "}\n",
    "class_to_states = {\n",
    "    0: 'all_growing',\n",
    "    1: 'partly_growing',\n",
    "    2: 'stopped_growing',\n",
    "    3: 'stopped_fading',\n",
    "    4: 'stopped_vanishing',\n",
    "    5: 'channel_empty'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fcec4",
   "metadata": {},
   "source": [
    "### Data creator using matplotlib widgets\n",
    "\n",
    "Mark the state transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8db7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateTransition(object):\n",
    "    \n",
    "    def __init__(self, phaseDir, fileformat='.tiff',\n",
    "                 labels_file='transitions.pickle', frame_rate=1):\n",
    "        \n",
    "        self.phaseDir = phaseDir\n",
    "        self.fileformat = fileformat\n",
    "        self.labelsFile = labels_file\n",
    "        self.frameRate = frame_rate\n",
    "        self.indices = [int(filename.split('.')[0].split('/')[-1]) for filename in \n",
    "                       glob.glob(self.phaseDir + \"*\" + self.fileformat)]\n",
    "        self.indices.sort()\n",
    "        \n",
    "        self.fig, self.ax = plt.subplots(1, 1, num=str(self.phaseDir), figsize=(12, 10))\n",
    "        plt.subplots_adjust(left=0.35, bottom=0.25)\n",
    "        self.axcolor = 'lightgoldenrodyellow'\n",
    "        \n",
    "        self.currentFrame = 0\n",
    "        self.imagesPlot = self.ax.imshow(self.__getitem__(self.currentFrame), cmap='gray')\n",
    "        self.ax.set_title(str(self.currentFrame) + \" _ \" + str(self.currentFrame + self.frameRate))\n",
    "        \n",
    "          \n",
    "        self.previousax =  plt.axes([0.5, 0.025, 0.1, 0.03])\n",
    "        self.previousButton = Button(self.previousax, 'Previous', color=self.axcolor, hovercolor='0.975')\n",
    "        self.previousButton.on_clicked(self.previousDatapoint)\n",
    "        \n",
    "        self.nextax =  plt.axes([0.65, 0.025, 0.1, 0.03])\n",
    "        self.nextButton = Button(self.nextax, 'Next', color=self.axcolor, hovercolor='0.975')\n",
    "        self.nextButton.on_clicked(self.nextDatapoint)\n",
    "        \n",
    "        self.saveax = plt.axes([0.8, 0.025, 0.1, 0.03])\n",
    "        self.saveButton = Button(self.saveax, 'Save', color=self.axcolor, hovercolor='0.875')\n",
    "        self.saveButton.on_clicked(self.save)\n",
    "        \n",
    "        \n",
    "        self.rax = plt.axes([0.05, 0.5, 0.20, 0.25], facecolor=self.axcolor)\n",
    "        self.radioToState = {\n",
    "            '0 : All Growing': 0,\n",
    "            '1 : Partly Growing': 1,\n",
    "            '2 : Stopped Growing': 2,\n",
    "            '3 : Stopped Fading' : 3,\n",
    "            '4 : Stopped Vanishing': 4,\n",
    "            '5 : Channel Empty' : 5\n",
    "        }\n",
    "        self.radio = RadioButtons(self.rax, ('0 : All Growing', '1 : Partly Growing',\n",
    "                                            '2 : Stopped Growing', '3 : Stopped Fading', \n",
    "                                            '4 : Stopped Vanishing', '5 : Channel Empty'))\n",
    "        #self.radio.on_clicked(self.changeState)\n",
    "        \n",
    "        \n",
    "        # the keys will be frame1_frame2 number and value is the state\n",
    "        self.states = {\n",
    "            \n",
    "        }\n",
    "        \n",
    "        \n",
    "        plt.pause(0.01)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if idx + self.frameRate < self.__len__() and idx >= 0:\n",
    "            phaseFilename1 = self.phaseDir + str(self.indices[idx]) + self.fileformat\n",
    "            phaseFilename2 = self.phaseDir + str(self.indices[idx+self.frameRate]) + self.fileformat\n",
    "            img1 = io.imread(phaseFilename1)\n",
    "            img2 = io.imread(phaseFilename2)\n",
    "            \n",
    "            full_img = np.concatenate((img1, img2), axis = 1)\n",
    "            return full_img\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def nextDatapoint(self, buttonPress):\n",
    "        key = str(self.currentFrame) + \"_\" + str(self.currentFrame + self.frameRate)\n",
    "        #print(self.radio.value_selected)\n",
    "        self.states[key] = self.radioToState[self.radio.value_selected]\n",
    "        \n",
    "        print(self.states)\n",
    "        \n",
    "        # get next Image if you are in range\n",
    "        self.currentFrame += 1\n",
    "        nextImage = self.__getitem__(self.currentFrame)\n",
    "        #print(nextImage.shape)\n",
    "        if nextImage is not None:\n",
    "            self.imagesPlot.set_data(nextImage)\n",
    "            self.ax.set_title(str(self.currentFrame) + \" _ \" + str(self.currentFrame + self.frameRate))\n",
    "            self.fig.canvas.draw()\n",
    "        else:\n",
    "            self.currentFrame -= 1\n",
    "            \n",
    "    def previousDatapoint(self, buttonPress):\n",
    "        key = str(self.currentFrame) + \"_\" + str(self.currentFrame + self.frameRate)\n",
    "        #print(self.radio.value_selected)\n",
    "        self.states[key] = self.radioToState[self.radio.value_selected]\n",
    "        \n",
    "        print(self.states)\n",
    "        \n",
    "        # get next Image if you are in range\n",
    "        self.currentFrame -= 1\n",
    "        previousImage = self.__getitem__(self.currentFrame)\n",
    "        #print(nextImage.shape)\n",
    "        if previousImage is not None:\n",
    "            self.imagesPlot.set_data(previousImage)\n",
    "            self.ax.set_title(str(self.currentFrame) + \" _ \" + str(self.currentFrame + self.frameRate))\n",
    "            self.fig.canvas.draw()\n",
    "        else:\n",
    "            self.currentFrame += 1\n",
    "    \n",
    "    def save(self, save):\n",
    "        print(self.states)\n",
    "        saveFilename = self.phaseDir + self.labelsFile\n",
    "        with open(saveFilename, 'wb') as f:\n",
    "            pickle.dump(self.states, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        print(f\"File save: {saveFilename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6484f49",
   "metadata": {},
   "source": [
    "### Generate training data one stack at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6e5a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phaseDir = '/home/pk/Documents/trainingData/deadalive1/40/'\n",
    "#markStates = StateTransition(phaseDir, frame_rate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e6665",
   "metadata": {},
   "source": [
    "#### Dataset bundling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e8521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class statesDataset(object):\n",
    "    \n",
    "    def __init__(self, phaseDirectoriesList, fileformat='.tiff', \n",
    "                 transforms=None, class_to_states = {\n",
    "                        0: 'all_growing', 1: 'partly_growing',\n",
    "                        2: 'stopped_growing',3: 'stopped_fading',\n",
    "                        4: 'stopped_vanishing',5: 'channel_empty'}):\n",
    "        self.phaseDirectoriesList = phaseDirectoriesList\n",
    "        self.fileformat = fileformat\n",
    "        self.transforms = transforms\n",
    "        self.classToStates = class_to_states\n",
    "        self.nClasses = len(class_to_states)\n",
    "        \n",
    "        self.data = {} # each class has it's value as key and the values is a list of 2 filename tuples\n",
    "        \n",
    "        self.all_data = []\n",
    "        \n",
    "        for directory in self.phaseDirectoriesList:\n",
    "            statesFilename = directory + 'transitions.pickle'\n",
    "            with open(statesFilename, 'rb') as file:\n",
    "                states = pickle.load(file)\n",
    "            for key, value in states.items():\n",
    "                file1 = directory + key.split('_')[0] + self.fileformat\n",
    "                file2 = directory + key.split('_')[1] + self.fileformat\n",
    "                if value in self.data:\n",
    "                    self.data[value].append((file1, file2))\n",
    "                else:\n",
    "                    self.data[value] = [(file1, file2)]\n",
    "                \n",
    "                self.all_data.append((file1, file2, value))\n",
    "        \n",
    "        classStats = self.classStatistics()\n",
    "        self.weights = np.ones(shape=(self.nClasses,))\n",
    "        for key in classStats:\n",
    "            self.weights[key] /= classStats[key]\n",
    "                    \n",
    "    \n",
    "    def __len__(self):\n",
    "        #length = 0\n",
    "        #for key in self.data:\n",
    "        #    length += len(self.data[key])\n",
    "        #return length\n",
    "        return len(self.all_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        datapoint = self.all_data[idx]\n",
    "        \n",
    "        img1 = io.imread(datapoint[0])\n",
    "        img2 = io.imread(datapoint[1])\n",
    "        label = datapoint[2]\n",
    "        \n",
    "        sample = {\n",
    "            'frame1': img1,\n",
    "            'frame2': img2,\n",
    "            'label': label\n",
    "        }\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            sample = self.transforms(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def classStatistics(self):\n",
    "        statistics = {}\n",
    "        for key in self.data:\n",
    "            statistics[key] = len(self.data[key])\n",
    "            \n",
    "        return statistics\n",
    "    \n",
    "    def calculateWeights(self):\n",
    "        pass\n",
    "    \n",
    "    def plotDatapoint(self, idx):\n",
    "        \n",
    "        datapoint = self.__getitem__(idx)\n",
    "        \n",
    "        pltImage = np.concatenate((datapoint['frame1'], datapoint['frame2']), axis = 1)\n",
    "        plt.figure()\n",
    "        plt.imshow(pltImage, cmap='gray')\n",
    "        plt.title(f\"Class: {datapoint['label']} -- {self.classToStates[datapoint['label']]}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531a023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tensorizeSample(object):\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        sample['frame1'] = torch.from_numpy(sample['frame1']).unsqueeze(0)\n",
    "        sample['frame2'] = torch.from_numpy(sample['frame2']).unsqueeze(0)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54ef66",
   "metadata": {},
   "source": [
    "phaseDirectoriesList = ['/home/pk/Documents/trainingData/deadalive1/' + str(i) + '/' for i in range(0, 41)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c486293",
   "metadata": {},
   "source": [
    "dataset = statesDataset(phaseDirectoriesList, transforms = tensorizeSample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fa248",
   "metadata": {},
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714e314",
   "metadata": {},
   "source": [
    "dataset.clasStatistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ca762d",
   "metadata": {},
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ecba8",
   "metadata": {},
   "source": [
    "dataset[0]['frame1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f68bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a9be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb5d13a3",
   "metadata": {},
   "source": [
    "### Net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de4d1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_relu_norm(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel=3, stride=1, padding=1):\n",
    "        super(conv_relu_norm, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=kernel, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(output_channels, output_channels, kernel_size=kernel, stride=stride, padding=padding),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class stateNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses=6):\n",
    "        super(stateNet, self).__init__()\n",
    "        self.conv1 = conv_relu_norm(1, 64)\n",
    "        self.conv2 = conv_relu_norm(64, 128)\n",
    "        self.conv3 = conv_relu_norm(128, 256)\n",
    "        \n",
    "        self.conv4 = conv_relu_norm(256, 256)\n",
    "        \n",
    "        self.fc6 = nn.Linear(76800, 1024)\n",
    "        self.imageFeatures = nn.Sequential(\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.outputLinear = nn.Sequential(\n",
    "                nn.Linear(2048, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, nClasses)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward_one(self, img):\n",
    "        # get to conv feature head for each image\n",
    "        \n",
    "        #print(f\"Image shape: {img.shape}\")\n",
    "        batch_size = img.shape[0]\n",
    "        \n",
    "        conv1 = self.conv1(img)\n",
    "        #print(f\"Conv1 shape: {conv1.shape}\")\n",
    "        pool1 = F.relu(F.max_pool2d(conv1, (2, 2)))\n",
    "        #print(f\"Pool1 shape: {pool1.shape}\")\n",
    "        \n",
    "        conv2 = self.conv2(pool1)\n",
    "        #print(f\"Conv2 shape: {conv2.shape}\")\n",
    "        pool2 = F.relu(F.max_pool2d(conv2, (2, 2)))\n",
    "        #print(f\"Pool2 shape: {pool2.shape}\")\n",
    "        \n",
    "        conv3 = self.conv3(pool2)\n",
    "        #print(f\"Conv3 shape: {conv3.shape}\")\n",
    "        pool3 = F.relu(F.max_pool2d(conv3, (2, 3)))\n",
    "        #print(f\"Pool3 shape: {pool3.shape}\")\n",
    "        \n",
    "        conv4 = self.conv4(pool3)\n",
    "        #print(f\"Conv4 shape: {conv4.shape}\")\n",
    "        conv4_reshaped = conv4.view(batch_size, -1)\n",
    "        #print(f\"Conv4 reshaped: {conv4_reshaped.shape}\")\n",
    "        \n",
    "        fc6 = F.relu(self.fc6(conv4_reshaped))\n",
    "        #print(f\"FC6 shape: {fc6.shape}\")\n",
    "        \n",
    "        out = self.imageFeatures(fc6)\n",
    "        #print(f\"Output shape: {out.shape}\")\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward(self, img1, img2):\n",
    "        # pass the sample image through the net seperately and then\n",
    "        \n",
    "        imgFeatures1 = self.forward_one(img1)\n",
    "        imgFeatures2 = self.forward_one(img2)\n",
    "        \n",
    "        # stack the image features\n",
    "        stackedFeatures = torch.cat((imgFeatures1, imgFeatures2), 1)\n",
    "        #print(stackedFeatures.shape)\n",
    "        \n",
    "        \n",
    "        netOutput = self.outputLinear(stackedFeatures)\n",
    "        #print(netOutput.shape)\n",
    "        \n",
    "        return netOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a342e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adfc6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "phaseDirectoriesList = ['/home/pk/Documents/trainingData/deadalive1/' + str(i) + '/' for i in range(0, 41)]\n",
    "dataset = statesDataset(phaseDirectoriesList, transforms = tensorizeSample())\n",
    "statedataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=6)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ea4eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 355, 2: 162, 3: 919, 4: 53, 5: 71, 1: 38}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d54b5378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0028169 , 0.02631579, 0.00617284, 0.00108814, 0.01886792,\n",
       "       0.01408451])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b98e7b1",
   "metadata": {},
   "source": [
    "databatch = next(iter(stateDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d70246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = stateNet(nClasses=6)\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(dataset.weights,dtype=torch.float))\n",
    "criterion.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.00005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aacd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548f3223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: ..  ..Avg.Loss: 1.5330058133602142\n",
      "Epoch 2: ..  ..Avg.Loss: 1.094512906074524\n",
      "Epoch 3: ..  ..Avg.Loss: 0.9336384582519531\n",
      "Epoch 4: ..  ..Avg.Loss: 0.7711231178045272\n",
      "Epoch 5: ..  ..Avg.Loss: 0.6599619603157043\n",
      "Epoch 6: ..  ..Avg.Loss: 0.6090902316570282\n",
      "Epoch 7: ..  ..Avg.Loss: 0.5522158116102218\n",
      "Epoch 8: ..  ..Avg.Loss: 0.3693456320464611\n",
      "Epoch 9: ..  ..Avg.Loss: 0.38225493013858797\n",
      "Epoch 10: ..  ..Avg.Loss: 0.3333915063738823\n"
     ]
    }
   ],
   "source": [
    "nEpochs = 10\n",
    "for epoch in range(nEpochs):\n",
    "    epoch_loss = []\n",
    "    for i_batch, data_batch in enumerate(statedataloader, 0):\n",
    "        frame1_batch, frame2_batch, labels = data_batch['frame1'].to(device), data_batch['frame2'].to(device), data_batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output_scores = net(frame1_batch, frame2_batch)\n",
    "        loss = criterion(output_scores, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}: ..  ..Avg.Loss: {np.mean(epoch_loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75240110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9125, -3.5484,  0.4680,  6.9903,  4.7184, -5.3253],\n",
       "        [ 3.4226, -0.5304,  2.9040,  2.2176,  0.2189, -5.4145],\n",
       "        [-0.0680, -4.9235, -0.8241,  5.5935,  3.8342, -2.7889],\n",
       "        [ 5.5589,  1.6421,  3.7758, -0.3839, -0.9987, -5.2551],\n",
       "        [ 0.7648, -2.8116,  3.7682,  7.5868,  1.8685, -7.4572],\n",
       "        [ 7.8441,  0.8090,  3.5499,  0.3334, -3.4195, -5.1678],\n",
       "        [-1.5301, -6.5270,  0.2636,  7.1894,  5.8312, -5.0786],\n",
       "        [ 3.5874, -2.8173,  3.2082,  2.5729,  0.8387, -5.0557],\n",
       "        [ 7.7557,  1.3497,  2.2764,  0.1000, -2.5069, -4.8078],\n",
       "        [-1.7252, -5.2836,  1.1182,  7.9258,  4.2648, -5.5359],\n",
       "        [ 4.0144,  0.6745,  4.1240,  0.7771,  0.1702, -5.4624],\n",
       "        [-1.8071, -6.2825, -1.1059,  7.4153,  5.0598, -3.5935],\n",
       "        [-0.7318, -5.7279, -0.6559,  7.1132,  4.2798, -3.3341],\n",
       "        [ 8.2844,  3.0437,  1.0200,  1.2183, -2.9872, -5.0463],\n",
       "        [ 8.0044,  0.6238,  0.6490,  1.8685, -2.4512, -5.0594],\n",
       "        [-0.6129, -5.2056,  1.7339, 10.2868,  3.1869, -7.4907],\n",
       "        [ 6.0812,  2.6175,  2.7252, -0.7607, -1.5389, -4.8684],\n",
       "        [ 2.8756, -1.5827,  0.7494,  3.5740,  1.3055, -4.6154],\n",
       "        [ 4.2448, -0.0709,  2.9985,  0.3068, -0.4549, -3.9714],\n",
       "        [ 1.0372, -1.7561,  2.0529,  5.8208,  1.1228, -5.1029],\n",
       "        [-1.0635, -5.1235, -0.4258,  5.0974,  5.3681, -2.8911],\n",
       "        [ 4.0576,  2.3551,  0.5825,  2.8405, -0.2984, -3.1474],\n",
       "        [-0.8413, -1.4419,  1.1614,  3.3113,  0.8469, -1.5382],\n",
       "        [-0.0888, -3.3714,  0.9760,  5.5415,  2.8752, -4.3963],\n",
       "        [-1.3659, -5.0536,  0.3866,  7.9848,  3.5623, -4.0284],\n",
       "        [ 5.6806,  1.5826,  3.7465, -0.2647, -1.6480, -4.7998],\n",
       "        [-1.0326, -6.0691, -0.6411,  6.8429,  4.7896, -3.7545],\n",
       "        [-0.6713, -3.4653,  2.9021,  8.3370,  4.2564, -7.9786],\n",
       "        [-1.0853, -5.8942, -0.6443,  7.3925,  4.3496, -4.2170],\n",
       "        [-5.2136, -4.9885, -1.0266,  4.0456, 10.1374, -2.0365]],\n",
       "       device='cuda:1', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b2f7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "squish = nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94885f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3455e-04, 2.3974e-05, 1.3305e-03, 9.0500e-01, 9.3311e-02, 4.0554e-06],\n",
       "        [5.1151e-01, 9.8193e-03, 3.0454e-01, 1.5329e-01, 2.0774e-02, 7.4289e-05],\n",
       "        [2.9529e-03, 2.2991e-05, 1.3864e-03, 8.4924e-01, 1.4620e-01, 1.9435e-04],\n",
       "        [8.3887e-01, 1.6698e-02, 1.4103e-01, 2.2017e-03, 1.1906e-03, 1.6874e-05],\n",
       "        [1.0615e-03, 2.9698e-05, 2.1393e-02, 9.7431e-01, 3.2008e-03, 2.8521e-07],\n",
       "        [9.8513e-01, 8.6733e-04, 1.3445e-02, 5.3907e-04, 1.2641e-05, 2.2003e-06],\n",
       "        [1.2984e-04, 8.7752e-07, 7.8055e-04, 7.9474e-01, 2.0435e-01, 3.7351e-06],\n",
       "        [4.7329e-01, 7.8271e-04, 3.2394e-01, 1.7161e-01, 3.0295e-02, 8.3463e-05],\n",
       "        [9.9370e-01, 1.6411e-03, 4.1458e-03, 4.7036e-04, 3.4693e-05, 3.4751e-06],\n",
       "        [6.2672e-05, 1.7852e-06, 1.0764e-03, 9.7382e-01, 2.5034e-02, 1.3871e-06],\n",
       "        [4.5208e-01, 1.6022e-02, 5.0444e-01, 1.7752e-02, 9.6761e-03, 3.4632e-05],\n",
       "        [9.0221e-05, 1.0272e-06, 1.8189e-04, 9.1311e-01, 8.6606e-02, 1.5117e-05],\n",
       "        [3.6962e-04, 2.5003e-06, 3.9878e-04, 9.4370e-01, 5.5497e-02, 2.7390e-05],\n",
       "        [9.9318e-01, 5.2603e-03, 6.9521e-04, 8.4774e-04, 1.2642e-05, 1.6127e-06],\n",
       "        [9.9655e-01, 6.2109e-04, 6.3694e-04, 2.1565e-03, 2.8689e-05, 2.1133e-06],\n",
       "        [1.8444e-05, 1.8677e-07, 1.9278e-04, 9.9896e-01, 8.2437e-04, 1.9006e-08],\n",
       "        [9.3654e-01, 2.9328e-02, 3.2661e-02, 1.0003e-03, 4.5935e-04, 1.6451e-05],\n",
       "        [2.9850e-01, 3.4573e-03, 3.5610e-02, 6.0017e-01, 6.2095e-02, 1.6659e-04],\n",
       "        [7.5201e-01, 1.0044e-02, 2.1625e-01, 1.4654e-02, 6.8419e-03, 2.0322e-04],\n",
       "        [8.0355e-03, 4.9191e-04, 2.2188e-02, 9.6051e-01, 8.7536e-03, 1.7314e-05],\n",
       "        [9.1068e-04, 1.5708e-05, 1.7231e-03, 4.3153e-01, 5.6568e-01, 1.4643e-04],\n",
       "        [6.5666e-01, 1.1966e-01, 2.0329e-02, 1.9443e-01, 8.4245e-03, 4.8778e-04],\n",
       "        [1.2745e-02, 6.9903e-03, 9.4430e-02, 8.1054e-01, 6.8945e-02, 6.3487e-03],\n",
       "        [3.3104e-03, 1.2424e-04, 9.6010e-03, 9.2278e-01, 6.4140e-02, 4.4580e-05],\n",
       "        [8.5817e-05, 2.1481e-06, 4.9511e-04, 9.8756e-01, 1.1854e-02, 5.9880e-06],\n",
       "        [8.5877e-01, 1.4260e-02, 1.2414e-01, 2.2482e-03, 5.6374e-04, 2.4115e-05],\n",
       "        [3.3645e-04, 2.1857e-06, 4.9764e-04, 8.8553e-01, 1.1361e-01, 2.2121e-05],\n",
       "        [1.1982e-04, 7.3303e-06, 4.2702e-03, 9.7906e-01, 1.6544e-02, 8.0360e-08],\n",
       "        [1.9847e-04, 1.6189e-06, 3.0847e-04, 9.5398e-01, 4.5504e-02, 8.6622e-06],\n",
       "        [2.1486e-07, 2.6909e-07, 1.4142e-05, 2.2560e-03, 9.9772e-01, 5.1514e-06]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squish(output_scores.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c5a036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf42c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedModel = {\n",
    "    'model_state_dict': net.state_dict(),\n",
    "    'labels': class_to_states\n",
    "}\n",
    "torch.save(savedModel, '/home/pk/Documents/models/multistate.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880d10b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b79539ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fff636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01aeefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5b7e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "squish = nn.Softmax(dim = 1)\n",
    "with torch.no_grad():\n",
    "    for i_batch, data_batch in enumerate(statedataloader, 0):\n",
    "        frame1_batch, frame2_batch, labels = data_batch['frame1'].to(device), data_batch['frame2'].to(device), data_batch['label'].to(device)\n",
    "        \n",
    "        output_scores = net(frame1_batch, frame2_batch)\n",
    "        output_scoresCPU = output_scores.cpu()\n",
    "        output_labels = squish(output_scoresCPU).numpy().argmax(axis = 1)\n",
    "        labels_cpu = labels.cpu().numpy()\n",
    "        \n",
    "        actual_labels.extend(list(labels_cpu))\n",
    "        predicted_labels.extend(list(output_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "172b2eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1598"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2acb1c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1598"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5712e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(actual_labels, predicted_labels, normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc195976",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.matshow(cm, cmap=plt.cm.Blues)\n",
    "ax.set_title('Classification errors')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "#ax.set_xticklabels([''] + labels)\n",
    "#ax.set_yticklabels([''] + labels)\n",
    "for (i, j), z in np.ndenumerate(cm):\n",
    "    ax.text(j, i, z, ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f8a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8956de0",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "441ba5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testData(object):\n",
    "    \n",
    "    def __init__(self, phaseDir, fileformat='.tiff'):\n",
    "        \n",
    "        self.phaseDir = phaseDir\n",
    "        self.fileformat = fileformat\n",
    "        self.indices = [int(filename.split('.')[0].split('/')[-1]) for filename in \n",
    "                       glob.glob(self.phaseDir + \"*\" + self.fileformat)]\n",
    "        self.indices.sort()\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.indices) - 1\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        phaseFilename1 = self.phaseDir + str(self.indices[idx]) + self.fileformat\n",
    "        phaseFilename2 = self.phaseDir + str(self.indices[idx+1]) + self.fileformat\n",
    "        img1 = io.imread(phaseFilename1)\n",
    "        img2 = io.imread(phaseFilename2)\n",
    "        print(phaseFilename1)\n",
    "        print(phaseFilename2)\n",
    "        \n",
    "        return {\n",
    "            'frame1': torch.from_numpy(img1).unsqueeze(0).unsqueeze(0),\n",
    "            'frame2': torch.from_numpy(img2).unsqueeze(0).unsqueeze(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "437d446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phaseTestDir = '/home/pk/Documents/trainingData/deadalive1/81/'\n",
    "testdata = testData(phaseTestDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1abc39af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stateNet(\n",
       "  (conv1): conv_relu_norm(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv2): conv_relu_norm(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): conv_relu_norm(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv4): conv_relu_norm(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (fc6): Linear(in_features=76800, out_features=1024, bias=True)\n",
       "  (imageFeatures): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (outputLinear): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=256, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = stateNet()\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0d24f34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_297752/1566842071.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'testdata' is not defined"
     ]
    }
   ],
   "source": [
    "out = net(testdata[0]['frame1'], testdata[0]['frame2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72b28170",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_297752/2594531152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b78b7bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1753, 0.1596, 0.1644, 0.1739, 0.1548, 0.1719]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squish = nn.Softmax(dim = 1)\n",
    "squish(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef81886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abe563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd37a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6bf5e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_states = {\n",
    "    0: 'all_growing',\n",
    "    1: 'partly_growing',\n",
    "    2: 'stopped_growing',\n",
    "    3: 'stopped_fading',\n",
    "    4: 'stopped_vanishing',\n",
    "    5: 'channel_empty'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4258abeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_growing\n",
      "partly_growing\n",
      "stopped_growing\n",
      "stopped_fading\n",
      "stopped_vanishing\n",
      "channel_empty\n"
     ]
    }
   ],
   "source": [
    "for key in class_to_states:\n",
    "    print(class_to_states[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e19744c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in class_to_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d10a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c5a26b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_to_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87d14ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'0_1'.split(\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbe81f",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2252c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005729b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06597165",
   "metadata": {},
   "source": [
    "### Test plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f111089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
